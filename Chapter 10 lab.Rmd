---
title: "Chapter 10 lab"
output:
  word_document: default
  html_document: default
---

In this lab, we perform PCA on the USArrests data set, which is part of
the base R package. The rows of the data set contain the 50 states, in
alphabetical order.
```{r}
states=row.names(USArrests )
states
```
```{r}
names(USArrests )
```

We first briefly examine the data. We notice that the variables have vastly
different means.
```{r}
apply(USArrests , 2, mean)
```

We can also examine the variances of the four variables using the apply()
function.
```{r}
apply(USArrests , 2, var)
```
We now perform principal components analysis using the prcomp() func- prcomp() tion, which is one of several functions in R that perform PCA.
```{r}
pr.out=prcomp(USArrests , scale=TRUE)
names(pr.out)
```
The center and scale components correspond to the means and standard
deviations of the variables that were used for scaling prior to implementing
PCA.

```{r}
pr.out$center

```
```{r}
pr.out$scale

```

The rotation matrix provides the principal component loadings; each column of pr.out$rotation contains the corresponding principal component
loading vector.2
```{r}
pr.out$rotation
```
```{r}
dim(pr.out$x)
```
We can plot the first two principal components as follows:
```{r}
biplot (pr.out , scale =0)
```

```{r}
pr.out$rotation=-pr.out$rotation
pr.out$x=-pr.out$x
biplot (pr.out , scale =0)
```

 USArrests data set, we can access
these standard deviations as follows:
```{r}
pr.out$sdev
```
```{r}
pr.var=pr.out$sdev ^2
pr.var
```
To compute the proportion of variance explained by each principal component, we simply divide the variance explained by each principal component
by the total variance explained by all four principal components:
```{r}
pve=pr.var/sum(pr.var)
pve
```
We see that the first principal component explains 62.0 % of the variance
in the data, the next principal component explains 24.7 % of the variance,
and so forth. We can plot the PVE explained by each component, as well
as the cumulative PVE, as follows:

```{r}
plot(pve , xlab=" Principal Component ", ylab="Proportion of
Variance Explained ", ylim=c(0,1),type='b')
```

```{r}
plot(cumsum(pve), xlab="Principal Component ", ylab="
Cumulative Proportion of Variance Explained ", ylim=c(0,1),
type='b')
```

```{r}
a=c(1,2,8,-3)
cumsum(a)
```

10.5 Lab 2: Clustering
10.5.1 K-Means Clustering
The function kmeans() performs K-means clustering in R. We begin with kmeans() a simple simulated example in which there truly are two clusters in the
data: the first 25 observations have a mean shift relative to the next 25
observations.
```{r}
set.seed(2)
x=matrix(rnorm (50*2), ncol=2)
x[1:25,1]=x[1:25,1]+3
x[1:25,2]=x[1:25,2]-4
```

```{r}
km.out=kmeans (x,2, nstart =20)
km.out$cluster
```

The K-means clustering perfectly separated the observations into two clusters even though we did not supply any group information to kmeans(). We
can plot the data, with each observation colored according to its cluster
assignment.
```{r}
plot(x, col=(km.out$cluster +1), main="K-Means Clustering
Results with K=2", xlab="", ylab="", pch=20, cex=2)
```



Here the observations can be easily plotted because they are two-dimensional.
If there were more than two variables then we could instead perform PCA
and plot the first two principal components score vectors.
In this example, we knew that there really were two clusters because
we generated the data. However, for real data, in general we do not know
the true number of clusters. We could instead have performed K-means
clustering on this example with K = 3.
```{r}
set.seed(4)
km.out=kmeans (x,3, nstart =20)
km.out
```
```{r}
plot(x, col=(km.out$cluster +1), main="K-Means Clustering
Results with K=3", xlab="", ylab="", pch=20, cex=2)
```



When K = 3, K-means clustering splits up the two clusters.
To run the kmeans() function in R with multiple initial cluster assignments, we use the nstart argument. If a value of nstart greater than one
is used, then K-means clustering will be performed using multiple random
assignments in Step 1 of Algorithm 10.1, and the kmeans() function will
report only the best results. Here we compare using nstart=1 to nstart=20.
```{r}
set.seed(3)
km.out=kmeans (x,3, nstart =1)
km.out$tot.withinss
```
```{r}

km.out=kmeans (x,3, nstart =20)
km.out$tot.withinss
```

The hclust() function implements hierarchical clustering in R. In the fol- hclust() lowing example we use the data from Section 10.5.1 to plot the hierarchical
clustering dendrogram using complete, single, and average linkage clustering, with Euclidean distance as the dissimilarity measure. We begin by
clustering observations using complete linkage. 
```{r}
hc.complete =hclust(dist(x), method="complete")
```

```{r}
hc.average =hclust(dist(x), method ="average")
hc.single=hclust(dist(x), method ="single")
```


We can now plot the dendrograms obtained using the usual plot() function.
The numbers at the bottom of the plot identify each observation.
```{r}
par(mfrow=c(1,3))
plot(hc.complete ,main="Complete Linkage ", xlab="", sub="",
cex=.9)
```

```{r}
plot(hc.average , main="Average Linkage", xlab="", sub="",
cex=.9)
plot(hc.single , main="Single Linkage ", xlab="", sub="",
cex=.9)
```

To determine the cluster labels for each observation associated with a
given cut of the dendrogram, we can use the cutree() function: cutree()

```{r}
cutree(hc.complete , 2)
```
```{r}
cutree(hc.average , 2)
```
```{r}
cutree(hc.single , 2)
```
For this data, complete and average linkage generally separate the observations into their correct groups. However, single linkage identifies one point
as belonging to its own cluster. A more sensible answer is obtained when
four clusters are selected, although there are still two singletons.

```{r}
cutree(hc.single , 4)
```
To scale the variables before performing hierarchical clustering of the
observations, we use the scale() function: scale()
```{r}
xsc=scale(x)
plot(hclust(dist(xsc), method ="complete"), main=" Hierarchical
Clustering with Scaled Features")
```
```{r}
x=matrix(rnorm (30*3), ncol=3)
dd=as.dist(1-cor(t(x)))
plot(hclust(dd, method ="complete"), main=" Complete Linkage
with Correlation -Based Distance ", xlab="", sub ="")
```



10.6 Lab 3: NCI60 Data Example
Unsupervised techniques are often used in the analysis of genomic data.
In particular, PCA and hierarchical clustering are popular tools. We illustrate these techniques on the NCI60 cancer cell line microarray data, which
consists of 6,830 gene expression measurements on 64 cancer cell lines.

```{r}
library(ISLR)
nci.labs=NCI60$labs
nci.data=NCI60$data
```


Each cell line is labeled with a cancer type. We do not make use of the
cancer types in performing PCA and clustering, as these are unsupervised
techniques. But after performing PCA and clustering, we will check to
see the extent to which these cancer types agree with the results of these
unsupervised techniques.
The data has 64 rows and 6,830 columns.
```{r}
dim(nci.data)
```
```{r}
nci.labs[1:4]
```
```{r}
table(nci.labs)
```

10.6.1 PCA on the NCI60 Data
We first perform PCA on the data after scaling the variables (genes) to
have standard deviation one, although one could reasonably argue that it
is better not to scale the genes.
```{r}
pr.out=prcomp(nci.data , scale=TRUE)
```

```{r}
Cols <- function(vec){
  cols <- rainbow(length(unique(vec)))
  return(cols[as.numeric(as.factor(vec))])
}
```




```{r}
par(mfrow=c(1,2))
plot(pr.out$x[,1:2], col=Cols(nci.labs), pch=19,
xlab="Z1",ylab="Z2")
plot(pr.out$x[,c(1,3)], col=Cols(nci.labs), pch=19,
xlab="Z1",ylab="Z3")
```


```{r}
summary (pr.out)
```


```{r}
plot(pr.out)
```

plot the PVE of each principal component (i.e. a scree plot) and the cumulative PVE of each principal component. This can be done with just a
little work.
```{r}
pve =100*pr.out$sdev ^2/sum(pr.out$sdev ^2)
par(mfrow=c(1,2))
plot(pve , type="o", ylab="PVE", xlab=" Principal Component ",
col="blue")
plot(cumsum(pve), type="o", ylab="Cumulative PVE", xlab="
Principal Component ", col="brown3")
```

```{r}
sd.data=scale(nci.data)
```

```{r}
par(mfrow=c(1,3))
data.dist=dist(sd.data)
plot(hclust(data.dist), labels=nci.labs , main="Complete
Linkage ", xlab="", sub="",ylab="")
plot(hclust(data.dist , method ="average"), labels=nci.labs ,
main="Average Linkage ", xlab="", sub="",ylab="")
plot(hclust(data.dist , method ="single"), labels=nci.labs ,
main="Single Linkage ", xlab="", sub="",ylab="")
```

clustering is not perfect. We will use complete linkage hierarchical clustering for the analysis that follows.
We can cut the dendrogram at the height that will yield a particular
number of clusters, say four:
```{r}
hc.out=hclust(dist(sd.data))
hc.clusters =cutree (hc.out ,4)
table(hc.clusters ,nci.labs)
```

There are some clear patterns. All the leukemia cell lines fall in cluster 3,
while the breast cancer cell lines are spread out over three different clusters.
We can plot the cut on the dendrogram that produces these four clusters:
```{r}
par(mfrow=c(1,1))
plot(hc.out , labels =nci.labs)
abline(h=139, col="red")
```



The abline() function draws a straight line on top of any existing plot
in R. The argument h=139 plots a horizontal line at height 139 on the dendrogram; this is the height that results in four distinct clusters. 
```{r}
hc.out
```
We claimed earlier in Section 10.3.2 that K-means clustering and hierarchical clustering with the dendrogram cut to obtain the same number
of clusters can yield very different results. How do these NCI60 hierarchical
clustering results compare to what we get if we perform K-means clustering
with K = 4?
```{r}
set.seed(2)
km.out=kmeans(sd.data , 4, nstart =20)
km.clusters =km.out$cluster
table(km.clusters ,hc.clusters )
```
```{r}
hc.out=hclust(dist(pr.out$x [,1:5]) )
plot(hc.out , labels =nci.labs , main="Hier. Clust. on First
Five Score Vectors ")

```





