---
title: "BINF702 Final Exam Spring 2023"
output: html_document
---
BINF702 Final Exam Spring 2023
This is the BINF702 open book and notes exam. The results of this exam are due back to me as a pdf or docx file by noon 5/1/23 midnight. Your document should include a statement of the problem, all R code, and all associated outputs including graphics. You are not allowed to seek help from other individuals be they in the class or outside of the class.Each problem is worth 10 points unless otherwise noted.


Problem 1 - Consider the Pima Indian data contained in the MASS package.Provide a table of of probability of correct classification results from a 1, 3, 5 nearest neighbor classifier. Train on Pima.tr and test on Pima.te.Be careful not to include the type column as this is effectively the class label.

```{r}
library(MASS)
```

```{r}
head(Pima.tr)
```

```{r}
head(Pima.te)
```

```{r}
#remove the "type" from both datasets
Pima_tr <- Pima.tr[, -8]
Pima_te <- Pima.te[, -8]
```

```{r}
library(class)
pred_k1 <- knn(train = Pima_tr, test = Pima_te, cl = Pima.tr[, 8], k = 1)
table(pred_k1, Pima.te[, 8])
```
```{r}
pred_k3 <- knn(train = Pima_tr, test = Pima_te, cl = Pima.tr[, 8], k = 3)
table(pred_k3, Pima.te[, 8])
```
```{r}
pred_k5 <- knn(train = Pima_tr, test = Pima_te, cl = Pima.tr[, 8], k = 5)
table(pred_k5, Pima.te[, 8])
```

Problem 2 - Return to the Pima Indian data and provide probability of correct clasification results using a support vector machine. Once again train on Pima.tr and test on Pima.te.

```{r}

# Train an SVM classifier on the training set
library(e1071)
svm_model <- svm(type ~ ., data = Pima.tr)

# Make predictions on the testing set using the trained SVM
pred <- predict(svm_model, newdata = Pima.te)

# Calculate the accuracy of the SVM classifier
acc <- mean(pred == Pima.te$type)

# Print the accuracy
cat(sprintf("Accuracy: %.2f%%\n", 100 * acc))

```
```{r}
table(pred, Pima.te[, 8])
```

```{r}
((197+58)/ (197+51+26+58))*100
```
Problem 3 - Repeat your analysis of problem 2 using Random Forests.

```{r}
# Train a Random Forest classifier on the training set
library(randomForest)
rf_model <- randomForest(type ~ ., data = Pima.tr)

# Make predictions on the testing set using the trained Random Forest
pred <- predict(rf_model, newdata = Pima.te)

# Calculate the accuracy of the Random Forest classifier
acc <- mean(pred == Pima.te$type)

# Print the accuracy
cat(sprintf("Accuracy: %.2f%%\n", 100 * acc))

```

Problem 4 - Return to your model of problem 3 and perform a variable importance plot based on MeanDecreaseGini.Identify the top 3 variables and discuss their biological relevance.

```{r}
# Extract variable importance scores
importance_scores <- importance(rf_model)

importance_scores

```
```{r}
varImpPlot(rf_model)
```
These variables have biological relevance in predicting diabetes risk. Glucose is a well-known biomarker for diabetes as it measures the amount of sugar in the blood. BMI, or body mass index, is a measure of body fat based on height and weight, and is often used as an indicator of obesity, a known risk factor for diabetes. Age is also an important risk factor for diabetes as the risk of developing diabetes increases as people age.




Problem 5 - Consider the wines dataset contained in the kohonen package.Provide side by side boxplots of the original wines data along with the wines data that has been subjected to a column wise standardizing transformation.
```{r}
install.packages("kohonen")
```
```{r}
library("kohonen")
```
```{r}
data("wines")
```
```{r}
par(mfrow = c (1,2))
boxplot(wines, main = "Original Wines Data")

wines_std <- scale(wines)

boxplot(wines_std, main = "Standardized Wines Data")
```


Problem 6 - In this problem we will use a new package to calculate the desired number of clusters in the wine data and then we will use this number along with hclust to create a cluster index which we will compare to the true labels indicating the region the wine grapes were grown in. [Hint - Please install the NbClust package and execute the following command to determine the number of clusters.]


no_of_Clusters = NbClust(wines.sc, distance = “euclidean”, min.nc = 2, max.nc = 10, method = “complete”, index =“all”)

```{r}
install.packages("NbClust")
```

```{r}
library("NbClust")
no_of_Clusters = NbClust(wines.sc, distance = "euclidean", min.nc = 2, max.nc = 10, method = "complete", index ="all")
```


Compare the obtained class labels to the true ones contained in vintages.
```{r}
# Print the results of NbClust
no_of_Clusters$Best.nc
no_of_Clusters$All.index
no_of_Clusters$Best.index

# Create a hierarchical clustering with complete linkage
hc <- hclust(dist(wines.sc), method = "complete")

# Cut the dendrogram into the desired number of clusters
clusters <- cutree(hc, k = 10)

# Compare the obtained class labels to the true ones
table(clusters, vintages)

```


Problem 7 - Repeat your analysis or problem 6 using kmeans. First run set.seed(0)

```{r}


# Determine the number of clusters using k-means
kmeans_obj <- kmeans(wines.sc, centers = 3, nstart = 25)

# Obtain the cluster labels
clusters <- kmeans_obj$cluster

# Compare the obtained class labels to the true ones
table(clusters, vintages)

```

Problem 8 - Perform principle components analysis on the standardized wine data using prcomp.[Hint - Remember the data has already been centered and scaled]. How many principle components are needed to capture roughly 89% of the variance?

```{r}
# perform principal components analysis on the standardized wine data
pca_wines <- prcomp(wines.sc)

# create a scree plot to visualize the proportion of variance explained by each principal component
plot(pca_wines)

# calculate the cumulative proportion of variance explained by each principal component
prop_var <- cumsum(pca_wines$sdev^2/sum(pca_wines$sdev^2))

# find the number of principle components needed to capture roughly 89% of the variance
n_pc <- min(which(prop_var >= 0.89))

# print the number of principle components
cat("Number of principle components needed to capture roughly 89% of the variance:", n_pc)

```




Problem 9 - Make a plot of the scaled wines observations in the first two principle components. Plot “o” as the symbol and use red colors for the Barbera, green for the Barolo, and black for the Grigolino. Add in a legend in the bottom left portion of the plot.

```{r}
# create a vector of colors corresponding to the different wine types
wine_colors <- c("red", "green", "black")

# create a vector of wine types corresponding to the different colors
wine_types <- unique(vintages)

# center and scale the wine data
#wines.sc <- scale(wines)

# perform principal components analysis on the standardized wine data
pca_wines <- prcomp(wines.sc)

# create a plot of the first two principal components with colored symbols
plot(pca_wines$x[, 1], pca_wines$x[, 2], type = "n", xlab = "PC1", ylab = "PC2")
points(pca_wines$x[vintages == wine_types[1], 1], pca_wines$x[vintages == wine_types[1], 2], col = wine_colors[1], pch = "o")
points(pca_wines$x[vintages == wine_types[2], 1], pca_wines$x[vintages == wine_types[2], 2], col = wine_colors[2], pch = "o")
points(pca_wines$x[vintages == wine_types[3], 1], pca_wines$x[vintages == wine_types[3], 2], col = wine_colors[3], pch = "o")

# add a legend in the bottom left portion of the plot
legend("bottomleft", legend = wine_types, col = wine_colors, pch = "o")

```




Problem 10 - In this problem we will perform an analysis of the sacled wines data using Gaussian mixture-based clustering. [Hint - We will be using the mclust package and patterning our analysis after the tutorial contained here https://cran.r-project.org/web/packages/mclust/vignettes/mclust.html]
Calculate the BIC values using the scaled wines data, call these wines.sc.BIC. plot these BIC values and using the summary method on the BIC object provide an interpretation as to which model is the best. Call Mclust to obtain a full model-based clustering solution on the scaled wines data, call it wines.sc.modl. You will be passing wines.sc to Mclust and setting x = wines.sc.BIC. Use the summary method to examine this object but do not set parameters = TRUE. Finally let’s compare the clustering obtained using Gaussian-based mixtures agains the ground truth. Ground truth will be the rows in the table as obtained from the vintages and the columns will be the mixture terms.

```{r}
install.packages("mclust")
```

```{r}
library(mclust)

# calculate BIC values
wines.sc.BIC <- mclustBIC(wines.sc, G = 1:9)

# plot BIC values
plot(wines.sc.BIC, type = "b")

# find best model using BIC
best.model <- which.min(wines.sc.BIC)
cat("Best number of clusters:", best.model, "\n")

# fit model using Mclust
wines.sc.modl <- Mclust(wines.sc, G = best.model, initialization = list(hcSteps = 0))

# summarize clustering results
summary(wines.sc.modl, parameters = FALSE)

# compare with true labels
table(vintages, wines.sc.modl$classification)



```


